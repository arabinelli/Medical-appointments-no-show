{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, json, os\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import xgboost as xgb\n",
    "import sklearn.metrics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some jupyter magic\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patientId</th>\n",
       "      <th>appointmentId</th>\n",
       "      <th>gender</th>\n",
       "      <th>scheduledDay</th>\n",
       "      <th>appointmentDay</th>\n",
       "      <th>age</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>scholarship</th>\n",
       "      <th>hypertension</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>...</th>\n",
       "      <th>appointmentDayDOW__Monday</th>\n",
       "      <th>appointmentDayDOW__Saturday</th>\n",
       "      <th>appointmentDayDOW__Thursday</th>\n",
       "      <th>appointmentDayDOW__Tuesday</th>\n",
       "      <th>appointmentDayDOW__Wednesday</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>distanceFromCenter</th>\n",
       "      <th>distanceFromCenterLat</th>\n",
       "      <th>distanceFromCenterLon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.987250e+13</td>\n",
       "      <td>5642903</td>\n",
       "      <td>F</td>\n",
       "      <td>2016-04-29 18:38:08</td>\n",
       "      <td>2016-04-29 00:00:00</td>\n",
       "      <td>62</td>\n",
       "      <td>JARDIM DA PENHA</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.490525</td>\n",
       "      <td>-0.169426</td>\n",
       "      <td>-0.198132</td>\n",
       "      <td>-0.280227</td>\n",
       "      <td>-0.201355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.589978e+14</td>\n",
       "      <td>5642503</td>\n",
       "      <td>M</td>\n",
       "      <td>2016-04-29 16:08:27</td>\n",
       "      <td>2016-04-29 00:00:00</td>\n",
       "      <td>56</td>\n",
       "      <td>JARDIM DA PENHA</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.490525</td>\n",
       "      <td>-0.169426</td>\n",
       "      <td>-0.198132</td>\n",
       "      <td>-0.280227</td>\n",
       "      <td>-0.201355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.262962e+12</td>\n",
       "      <td>5642549</td>\n",
       "      <td>F</td>\n",
       "      <td>2016-04-29 16:19:04</td>\n",
       "      <td>2016-04-29 00:00:00</td>\n",
       "      <td>62</td>\n",
       "      <td>MATA DA PRAIA</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.788316</td>\n",
       "      <td>-0.166879</td>\n",
       "      <td>-0.190882</td>\n",
       "      <td>0.089317</td>\n",
       "      <td>-0.199223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.679512e+11</td>\n",
       "      <td>5642828</td>\n",
       "      <td>F</td>\n",
       "      <td>2016-04-29 17:29:31</td>\n",
       "      <td>2016-04-29 00:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>PONTAL DE CAMBURI</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341636</td>\n",
       "      <td>-0.170699</td>\n",
       "      <td>-0.201469</td>\n",
       "      <td>-0.464992</td>\n",
       "      <td>-0.200074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.841186e+12</td>\n",
       "      <td>5642494</td>\n",
       "      <td>F</td>\n",
       "      <td>2016-04-29 16:07:23</td>\n",
       "      <td>2016-04-29 00:00:00</td>\n",
       "      <td>56</td>\n",
       "      <td>JARDIM DA PENHA</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.490525</td>\n",
       "      <td>-0.169426</td>\n",
       "      <td>-0.198132</td>\n",
       "      <td>-0.280227</td>\n",
       "      <td>-0.201355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      patientId  appointmentId gender         scheduledDay  \\\n",
       "0  2.987250e+13        5642903      F  2016-04-29 18:38:08   \n",
       "1  5.589978e+14        5642503      M  2016-04-29 16:08:27   \n",
       "2  4.262962e+12        5642549      F  2016-04-29 16:19:04   \n",
       "3  8.679512e+11        5642828      F  2016-04-29 17:29:31   \n",
       "4  8.841186e+12        5642494      F  2016-04-29 16:07:23   \n",
       "\n",
       "        appointmentDay  age       neighborhood  scholarship  hypertension  \\\n",
       "0  2016-04-29 00:00:00   62    JARDIM DA PENHA        False          True   \n",
       "1  2016-04-29 00:00:00   56    JARDIM DA PENHA        False         False   \n",
       "2  2016-04-29 00:00:00   62      MATA DA PRAIA        False         False   \n",
       "3  2016-04-29 00:00:00    8  PONTAL DE CAMBURI        False         False   \n",
       "4  2016-04-29 00:00:00   56    JARDIM DA PENHA        False          True   \n",
       "\n",
       "   diabetes  ...  appointmentDayDOW__Monday  appointmentDayDOW__Saturday  \\\n",
       "0     False  ...                          0                            0   \n",
       "1     False  ...                          0                            0   \n",
       "2     False  ...                          0                            0   \n",
       "3     False  ...                          0                            0   \n",
       "4      True  ...                          0                            0   \n",
       "\n",
       "   appointmentDayDOW__Thursday  appointmentDayDOW__Tuesday  \\\n",
       "0                            0                           0   \n",
       "1                            0                           0   \n",
       "2                            0                           0   \n",
       "3                            0                           0   \n",
       "4                            0                           0   \n",
       "\n",
       "   appointmentDayDOW__Wednesday       lat       lon distanceFromCenter  \\\n",
       "0                             0  0.490525 -0.169426          -0.198132   \n",
       "1                             0  0.490525 -0.169426          -0.198132   \n",
       "2                             0  0.788316 -0.166879          -0.190882   \n",
       "3                             0  0.341636 -0.170699          -0.201469   \n",
       "4                             0  0.490525 -0.169426          -0.198132   \n",
       "\n",
       "   distanceFromCenterLat  distanceFromCenterLon  \n",
       "0              -0.280227              -0.201355  \n",
       "1              -0.280227              -0.201355  \n",
       "2               0.089317              -0.199223  \n",
       "3              -0.464992              -0.200074  \n",
       "4              -0.280227              -0.201355  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_show_df = pd.read_csv(\"data/no_show_feature_engineered_no_extreme_locations.csv\")\n",
    "no_show_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change how gender is encoded\n",
    "- Select features\n",
    "- Define the baseline model\n",
    "- Define xgboost model\n",
    "- Define tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the gender as a binary\n",
    "# NOTE: the gender didn't seem to affect no show by itself, but we're going to keep it and\n",
    "#       and verify if the model can still use it in conjunction with other variables\n",
    "no_show_df[\"isFemale\"] = no_show_df[\"gender\"] == \"F\"\n",
    "\n",
    "# select the columns that we want to keep\n",
    "FEATURE_COLS = [\"age\",\"scholarship\",\"hypertension\",\"diabetes\",\"alcoholism\",\"handicap\",\"smsSent\",\n",
    "                \"daysInAdvance\",\"lat\",\"lon\",\"isFemale\",\"distanceFromCenterLat\",\"scheduledDayHour\",\n",
    "                \"otherAppointmentsOnSameDay\",\"previouslyMissed\"]\n",
    "days_of_weeks_cols = [col_name for col_name in no_show_df.columns if \"appointmentDayDOW__\" in col_name]\n",
    "FEATURE_COLS += days_of_weeks_cols\n",
    "\n",
    "# target column\n",
    "TARGET_COLUMN = \"noShow\"\n",
    "\n",
    "# prepare dataset for models\n",
    "no_show_df[\"age\"] = (no_show_df[\"age\"]-no_show_df[\"age\"].mean())/no_show_df[\"age\"].std()\n",
    "\n",
    "no_show_df[\"daysInAdvance\"] = (no_show_df[\"daysInAdvance\"]-no_show_df[\"daysInAdvance\"].min())/\\\n",
    "                                (no_show_df[\"daysInAdvance\"].max() - no_show_df[\"daysInAdvance\"].min())\n",
    "\n",
    "no_show_df[\"scheduledDayHour\"] = (no_show_df[\"scheduledDayHour\"]-no_show_df[\"scheduledDayHour\"].min())/\\\n",
    "                                (no_show_df[\"scheduledDayHour\"].max() - no_show_df[\"scheduledDayHour\"].min())\n",
    "\n",
    "no_show_df[\"previouslyMissed\"] = (no_show_df[\"previouslyMissed\"]-no_show_df[\"previouslyMissed\"].min())/\\\n",
    "                                (no_show_df[\"previouslyMissed\"].max() - no_show_df[\"previouslyMissed\"].min())\n",
    "\n",
    "no_show_df[\"noShow\"] = (no_show_df[\"noShow\"])*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>daysInAdvance</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>distanceFromCenterLat</th>\n",
       "      <th>scheduledDayHour</th>\n",
       "      <th>otherAppointmentsOnSameDay</th>\n",
       "      <th>previouslyMissed</th>\n",
       "      <th>appointmentDayDOW__Friday</th>\n",
       "      <th>appointmentDayDOW__Monday</th>\n",
       "      <th>appointmentDayDOW__Saturday</th>\n",
       "      <th>appointmentDayDOW__Thursday</th>\n",
       "      <th>appointmentDayDOW__Tuesday</th>\n",
       "      <th>appointmentDayDOW__Wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.104830e+05</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "      <td>110483.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.476613e-16</td>\n",
       "      <td>0.056905</td>\n",
       "      <td>0.271553</td>\n",
       "      <td>-0.183075</td>\n",
       "      <td>-0.101447</td>\n",
       "      <td>0.318306</td>\n",
       "      <td>0.146611</td>\n",
       "      <td>0.022802</td>\n",
       "      <td>0.172017</td>\n",
       "      <td>0.205552</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.156006</td>\n",
       "      <td>0.232009</td>\n",
       "      <td>0.234063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.085233</td>\n",
       "      <td>0.602535</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>0.427408</td>\n",
       "      <td>0.214422</td>\n",
       "      <td>0.353719</td>\n",
       "      <td>0.044387</td>\n",
       "      <td>0.377397</td>\n",
       "      <td>0.404106</td>\n",
       "      <td>0.018785</td>\n",
       "      <td>0.362863</td>\n",
       "      <td>0.422116</td>\n",
       "      <td>0.423414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.604708e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.629942</td>\n",
       "      <td>-0.209515</td>\n",
       "      <td>-0.743123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-8.258412e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.263382</td>\n",
       "      <td>-0.193611</td>\n",
       "      <td>-0.424844</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-3.703810e-03</td>\n",
       "      <td>0.022346</td>\n",
       "      <td>0.212190</td>\n",
       "      <td>-0.183430</td>\n",
       "      <td>-0.164289</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.751632e-01</td>\n",
       "      <td>0.083799</td>\n",
       "      <td>0.698255</td>\n",
       "      <td>-0.173670</td>\n",
       "      <td>0.168520</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.371386e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.290402</td>\n",
       "      <td>-0.149047</td>\n",
       "      <td>0.712382</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age  daysInAdvance            lat            lon  \\\n",
       "count  1.104830e+05  110483.000000  110483.000000  110483.000000   \n",
       "mean   1.476613e-16       0.056905       0.271553      -0.183075   \n",
       "std    1.000000e+00       0.085233       0.602535       0.014870   \n",
       "min   -1.604708e+00       0.000000      -0.629942      -0.209515   \n",
       "25%   -8.258412e-01       0.000000      -0.263382      -0.193611   \n",
       "50%   -3.703810e-03       0.022346       0.212190      -0.183430   \n",
       "75%    7.751632e-01       0.083799       0.698255      -0.173670   \n",
       "max    3.371386e+00       1.000000       1.290402      -0.149047   \n",
       "\n",
       "       distanceFromCenterLat  scheduledDayHour  otherAppointmentsOnSameDay  \\\n",
       "count          110483.000000     110483.000000               110483.000000   \n",
       "mean               -0.101447          0.318306                    0.146611   \n",
       "std                 0.427408          0.214422                    0.353719   \n",
       "min                -0.743123          0.000000                    0.000000   \n",
       "25%                -0.424844          0.133333                    0.000000   \n",
       "50%                -0.164289          0.266667                    0.000000   \n",
       "75%                 0.168520          0.466667                    0.000000   \n",
       "max                 0.712382          1.000000                    1.000000   \n",
       "\n",
       "       previouslyMissed  appointmentDayDOW__Friday  appointmentDayDOW__Monday  \\\n",
       "count     110483.000000              110483.000000              110483.000000   \n",
       "mean           0.022802                   0.172017                   0.205552   \n",
       "std            0.044387                   0.377397                   0.404106   \n",
       "min            0.000000                   0.000000                   0.000000   \n",
       "25%            0.000000                   0.000000                   0.000000   \n",
       "50%            0.000000                   0.000000                   0.000000   \n",
       "75%            0.055556                   0.000000                   0.000000   \n",
       "max            1.000000                   1.000000                   1.000000   \n",
       "\n",
       "       appointmentDayDOW__Saturday  appointmentDayDOW__Thursday  \\\n",
       "count                110483.000000                110483.000000   \n",
       "mean                      0.000353                     0.156006   \n",
       "std                       0.018785                     0.362863   \n",
       "min                       0.000000                     0.000000   \n",
       "25%                       0.000000                     0.000000   \n",
       "50%                       0.000000                     0.000000   \n",
       "75%                       0.000000                     0.000000   \n",
       "max                       1.000000                     1.000000   \n",
       "\n",
       "       appointmentDayDOW__Tuesday  appointmentDayDOW__Wednesday  \n",
       "count               110483.000000                 110483.000000  \n",
       "mean                     0.232009                      0.234063  \n",
       "std                      0.422116                      0.423414  \n",
       "min                      0.000000                      0.000000  \n",
       "25%                      0.000000                      0.000000  \n",
       "50%                      0.000000                      0.000000  \n",
       "75%                      0.000000                      0.000000  \n",
       "max                      1.000000                      1.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_show_df[FEATURE_COLS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training, validation, and test after \n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "no_show_df = no_show_df.sort_values(['appointmentDay','scheduledDay'])\n",
    "\n",
    "X = no_show_df[FEATURE_COLS].values\n",
    "y = no_show_df[TARGET_COLUMN].values\n",
    "\n",
    "SEQUENTIAL_SPLIT = False\n",
    "# get train = 60%, validation = 20%, test = 20%\n",
    "if not SEQUENTIAL_SPLIT:\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.2,random_state=127) # split between train and test\n",
    "    train_X, val_X, train_y, val_y = train_test_split(train_X,train_y,train_size=0.75,random_state=127) # split train to get validation\n",
    "\n",
    "else:\n",
    "    train_X = X[:int(X.shape[0]*(0.8))]\n",
    "    train_y = y[:int(y.shape[0]*(0.8))]\n",
    "    test_X = X[int(X.shape[0]*(0.8)):]\n",
    "    test_y = y[int(y.shape[0]*(0.8)):]\n",
    "\n",
    "    val_X = train_X[int(train_X.shape[0]*(0.75)):]\n",
    "    val_y = train_y[int(train_y.shape[0]*(0.75)):]\n",
    "\n",
    "    # remove validation from training set\n",
    "    train_X = train_X[:int(train_X.shape[0]*(0.75))]\n",
    "    train_y = train_y[:int(train_y.shape[0]*(0.75))]\n",
    "\n",
    "    assert len(train_X) + len(val_X) + len(test_X) == len(X), \"Something went wrong while splitting up the sets\"\n",
    "# using RandomOverSampler for generating synthetic samples to help with class imbalance\n",
    "oversample = RandomOverSampler(random_state=127)\n",
    "train_X, train_y = oversample.fit_resample(train_X, train_y)\n",
    "\n",
    "\n",
    "train_X = train_X.astype(np.float32)\n",
    "val_X = val_X.astype(np.float32)\n",
    "test_X = test_X.astype(np.float32)\n",
    "train_y = train_y.astype(np.int8)\n",
    "val_y = val_y.astype(np.int8)\n",
    "test_y = test_y.astype(np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does success look like?\n",
    "\n",
    "### Selecting some metrics\n",
    "As we assessed in the data exploration piece, we are facing a binary classification problem with an imbalanced dataset. For this reason, accuracy is not a good candidate to assess the performance of the models that we will build. Nonetheless, we will keep track of it as a vanity metric, to help understand how many misclassified cases the hospital will face (and, let's be honest, who isn't curious to know of how accurate the model is - regardless of it being a good metric or not?)\n",
    "\n",
    "The metrics that will be considered to assess which of the three proposed models is best at predicting no shows are:\n",
    "* **AUC ROC** - The area under the receiver operating characteristic curve considers the True Positive Rate (TPR = True Positives/(True Positives + False Negatives)) and the False Positive Rate (FPR = False Positives/(False Positives + True Negatives)). It gives us an indication of how good the model is at **predicting the positives as positive and negatives as negative**. \n",
    "* **Precision** - Represents the percentage of the values that the model flags as positive that are actually positive. \n",
    "* **Recall** - Represents the percentage of actually positive values that the model classify correctly.\n",
    "\n",
    "### Interpreting the metrics\n",
    "\n",
    "To understand how to interpret the metrics in the context of hospitals no-show, let's imagine for a moment that the hospitals will take our final model (chosen between the three here proposed) and set up a team which is going to be in charge of chasing the patients flagged by the model as potential no-shows.\n",
    "\n",
    "#### Precision\n",
    "In this case, precision is going to give us an indication of **wasted resources** when the hospitals follow the model recommendations. For example, a model with a 90% precision will mean that 9 patients out of 10 flagged by the model were in fact not going to show their appointments. This allows our hypothetical team to dedicate more resources \"chasing\" them, as they are extremely likely to actually result in no-shows. On contrary, a model scoring 30% in precision would mean that only 3 of those 10 patients flagged by the model were going to not show up. For our imaginary team, this means that in 7 cases out of 10 their effort was unnecessary (as the patients they were chasing were going to show up anyway).\n",
    "\n",
    "#### Recall\n",
    "Sticking with our imaginary 'no-show'-busters team, recall would represent their **peace of mind**. We can imagine that the model that we are building is going to be only one of the tools at the team disposal (e.g. they might want to decide to invest in a communication campaign around no-show awareness), and that the hospital management will evaluate the performance of the team on KPIs such as _\"Reduction in year-on-year number of no-shows in the hospitals of Vitoria\"_. Recall measures how many actual positives where identified by the model and, as a consequence, how many cases a team which blindly relies on the model is going to miss. For example, a model with a recall score of 30% will flag on average only 3 non-showing patients out of 10 - the remaining 7 will be flagges as showing and would not be surfaced to the attention of our busy team. \n",
    "\n",
    "#### AUC ROC\n",
    "This concept is a bit less intuitive, as the AUC ROC metrics evaluates the true positive and false positive rate at every possible threshold. While this sounds complicated at first, it becomes much simpler to graps with an understanding of how out machine learning estimators work. For a classification task, the models that we are using in this experiment do not output a yes/no answer, but rather a number between 0 and 1 which can be intuitively interpreted as the probability of a given appointment to result in a show or no-show. This means that we can define a threshold of what we consider to be a \"yes\" and what a \"no\" - for the standard precision and recall metrics, this threshold is set at 0.5: lower than that, the patient shows up, higher than 0.5 the patient will not show up. **Therefore, what is defined as a true/false positive and true/false negative depends on this threshold** and the AUC ROC metric uses some magic (actually just math) to calculate how these values change at each threshold value between 0 and 1. Intuitively, the AUC ROC can be interpreted by **how good is the model at ranking the probabilities of actual positives higher than the probabilities of actual negatives**.\n",
    "\n",
    "#### TL;DR\n",
    "* A high precision means that we'll be focusing our efforts where they are actually needed\n",
    "* A high recall means that we don't have to worry about the model missing non-showing patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "As reported in publications studying hospitals no-shows, most clinics currently do not have any way to identify patients that are likely not to show up to appointments (Srinivas, Sharan, and A. Ravi Ravindran, 2018). For this reason, and given the class imbalance of the dataset, we can assume they naively decide on either assuming everyone will show up, or that they won't. \n",
    "\n",
    "From an accuracy perspective, the baseline model assuming that every patient always shows up gives the highest value (as most patients actually show up). In practice, even if hospitals cannot predict if a given patient will show up or not, they are likely to know a percentage of no-shows per day and use it to overbook appointments. \n",
    "\n",
    "When we focus on designing and implementing tools and processes to reduce no-shows (e.g. SMS reminders), we can assume that hospitals will consider every patient as potential no-show. (NOTE: in the case of SMS, we know that the hospitals do not send them to everyone. However, we will assume this to be unrelated to the way they manage no-shows, and to be related to other additional factors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_baseline_model(ground_truth,naive_choice=1):\n",
    "    \"\"\"Compute the metrics for the baseline model\"\"\"\n",
    "    pred_labels = [naive_choice]*len(ground_truth) \n",
    "    results = {\"accuracy\": sklearn.metrics.accuracy_score(ground_truth,pred_labels),\n",
    "             \"precision\": sklearn.metrics.precision_score(ground_truth,pred_labels),\n",
    "             \"recall\": sklearn.metrics.recall_score(ground_truth,pred_labels),\n",
    "             \"AUC_ROC\": sklearn.metrics.roc_auc_score(ground_truth,pred_labels)}\n",
    "    for metric, value in results.items():\n",
    "        print(metric,\"-\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy - 0.20061546816309905\n",
      "precision - 0.20061546816309905\n",
      "recall - 1.0\n",
      "AUC_ROC - 0.5\n"
     ]
    }
   ],
   "source": [
    "evaluate_baseline_model(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Wide and Deep model\n",
    "\n",
    "The wide and Deep model has been defined in the file 'models/deep_and_wide.py' using Tensorflow Keras Subclassing API. This model architecture was first designed by a team of Google researchers in the publication [Wide & Deep Learning for Recommender Systems](https://arxiv.org/abs/1606.07792).\n",
    "\n",
    "The model is named \"deep and wide\" after its architecture. It is made of two parts: a \"deep\" one, containing one or more dense hidden layers, which focuses on generalizing learnings in an abstract manner (e.g. birds can fly). The \"wide\" component simply connects the input straight to the output layer, skipping the hidden layers. This component focuses on memorization (e.g. penguins don't fly). The two components are then concatenated together before the output layer, which has 1 neuron and a sigmoid activation function to squeeze the output value of the neuron between 0 and 1. The model uses an Adam optimizer.\n",
    "\n",
    "The tuning of the hyperparameters has been performed with an [Asyncronous Hyperband](https://openreview.net/forum?id=S1Y7OOlRZ) algorithm supported by an hyperparameters search algorithm based on Tree-structured Parzen Estimators. In this notebook, it is only reported the final configuration of the network for brevity. The hyperparameter tuning code and all of its dependencies can be found in the file 'models/nn_hparams_tuning.py'. The metric being considered in this optimization process was the validation AUC ROC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import tensorflow as tf\n",
      "from tensorflow import keras\n",
      "from kerastuner.tuners import Hyperband\n",
      "\n",
      "\n",
      "class DeepAndWide(keras.Model):\n",
      "\n",
      "    def __init__(self,hidden_dim=64,activation=\"relu\",dropout=0.3,n_hidden_layers=2,regularization=0.001,**kwargs):\n",
      "        super().__init__(**kwargs)\n",
      "        self.hidden = keras.layers.Dense(hidden_dim,activation=activation, \n",
      "                                        kernel_regularizer=keras.regularizers.l2(regularization),name=\"Hidden\")\n",
      "        self.output_layer = keras.layers.Dense(1,activation=tf.nn.sigmoid)\n",
      "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
      "        self.n_hidden_layers = n_hidden_layers\n",
      "    \n",
      "    def call(self,inputs,training=False):\n",
      "        inputs\n",
      "        for _ in range(self.n_hidden_layers):\n",
      "            x2 = self.hidden(inputs)\n",
      "            x2 = self.dropout(x2, training=training)\n",
      "        x = keras.layers.concatenate([inputs,x2])\n",
      "        output = self.output_layer(x)\n",
      "        return output\n"
     ]
    }
   ],
   "source": [
    "from models.deep_and_wide import DeepAndWide\n",
    "\n",
    "!pygmentize models/deep_and_wide.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.deep_and_wide import DeepAndWide\n",
    "def make_nn_model(config):\n",
    "    \"\"\"Creates and compiles an instance of the model with the provided hyperparameters\"\"\"\n",
    "    model = DeepAndWide(hidden_dim=config[\"hidden_dim\"],\n",
    "                        activation=config[\"activation\"],\n",
    "                        dropout=0.3,\n",
    "                        n_hidden_layers=config[\"n_hidden_layers\"],\n",
    "                        regularization=config[\"regularization\"])\n",
    "    model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = config[\"lr\"]),\n",
    "    metrics=[\"accuracy\",tf.keras.metrics.AUC(curve=\"ROC\"),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration for the Deep and Wide network:\n",
      "{'activation': 'relu', 'hidden_dim': 45, 'iterations': 100, 'lr': 0.2975559558171681, 'n_hidden_layers': 1, 'regularization': 0.05059536754175111, 'threads': 2}\n",
      "Train on 105678 samples, validate on 22097 samples\n",
      "Epoch 1/60\n",
      "105678/105678 - 9s - loss: 0.3462 - accuracy: 0.9115 - auc: 0.9415 - precision: 0.8836 - recall: 0.9479 - val_loss: 0.4153 - val_accuracy: 0.8972 - val_auc: 0.9451 - val_precision: 0.6620 - val_recall: 0.9946\n",
      "Epoch 2/60\n",
      "105678/105678 - 6s - loss: 0.3284 - accuracy: 0.9210 - auc: 0.9442 - precision: 0.8903 - recall: 0.9602 - val_loss: 0.4069 - val_accuracy: 0.9007 - val_auc: 0.9453 - val_precision: 0.6777 - val_recall: 0.9618\n",
      "Epoch 3/60\n",
      "105678/105678 - 7s - loss: 0.3288 - accuracy: 0.9215 - auc: 0.9446 - precision: 0.8909 - recall: 0.9605 - val_loss: 0.4113 - val_accuracy: 0.8962 - val_auc: 0.9479 - val_precision: 0.6614 - val_recall: 0.9876\n",
      "Epoch 4/60\n",
      "105678/105678 - 7s - loss: 0.3269 - accuracy: 0.9210 - auc: 0.9447 - precision: 0.8904 - recall: 0.9603 - val_loss: 0.3830 - val_accuracy: 0.8995 - val_auc: 0.9471 - val_precision: 0.6736 - val_recall: 0.9670\n",
      "Epoch 5/60\n",
      "105678/105678 - 7s - loss: 0.3266 - accuracy: 0.9214 - auc: 0.9447 - precision: 0.8907 - recall: 0.9607 - val_loss: 0.3502 - val_accuracy: 0.9000 - val_auc: 0.9473 - val_precision: 0.6778 - val_recall: 0.9548\n",
      "Epoch 6/60\n",
      "105678/105678 - 6s - loss: 0.3256 - accuracy: 0.9224 - auc: 0.9451 - precision: 0.8906 - recall: 0.9631 - val_loss: 0.3827 - val_accuracy: 0.8983 - val_auc: 0.9461 - val_precision: 0.6680 - val_recall: 0.9790\n",
      "Epoch 7/60\n",
      "105678/105678 - 7s - loss: 0.3276 - accuracy: 0.9220 - auc: 0.9442 - precision: 0.8910 - recall: 0.9618 - val_loss: 0.3796 - val_accuracy: 0.9003 - val_auc: 0.9461 - val_precision: 0.6762 - val_recall: 0.9641\n",
      "Epoch 8/60\n",
      "105678/105678 - 7s - loss: 0.3266 - accuracy: 0.9216 - auc: 0.9447 - precision: 0.8906 - recall: 0.9614 - val_loss: 0.4624 - val_accuracy: 0.8947 - val_auc: 0.9448 - val_precision: 0.6556 - val_recall: 0.9993\n",
      "Epoch 9/60\n",
      "105678/105678 - 8s - loss: 0.3259 - accuracy: 0.9217 - auc: 0.9449 - precision: 0.8908 - recall: 0.9613 - val_loss: 0.4968 - val_accuracy: 0.8903 - val_auc: 0.9451 - val_precision: 0.6497 - val_recall: 0.9819\n",
      "Epoch 10/60\n",
      "105678/105678 - 6s - loss: 0.3266 - accuracy: 0.9215 - auc: 0.9445 - precision: 0.8911 - recall: 0.9603 - val_loss: 0.4465 - val_accuracy: 0.8985 - val_auc: 0.9437 - val_precision: 0.6735 - val_recall: 0.9577\n",
      "Epoch 11/60\n",
      "105678/105678 - 7s - loss: 0.3275 - accuracy: 0.9212 - auc: 0.9446 - precision: 0.8911 - recall: 0.9596 - val_loss: 0.3988 - val_accuracy: 0.8960 - val_auc: 0.9440 - val_precision: 0.6589 - val_recall: 0.9962\n",
      "Epoch 12/60\n",
      "105678/105678 - 7s - loss: 0.3276 - accuracy: 0.9207 - auc: 0.9448 - precision: 0.8906 - recall: 0.9593 - val_loss: 0.4335 - val_accuracy: 0.8964 - val_auc: 0.9424 - val_precision: 0.6622 - val_recall: 0.9855\n",
      "Epoch 13/60\n",
      "105678/105678 - 6s - loss: 0.3258 - accuracy: 0.9223 - auc: 0.9451 - precision: 0.8914 - recall: 0.9617 - val_loss: 0.3638 - val_accuracy: 0.8946 - val_auc: 0.9436 - val_precision: 0.6930 - val_recall: 0.8507\n",
      "Epoch 14/60\n",
      "105678/105678 - 6s - loss: 0.3282 - accuracy: 0.9208 - auc: 0.9444 - precision: 0.8907 - recall: 0.9595 - val_loss: 0.4051 - val_accuracy: 0.8987 - val_auc: 0.9442 - val_precision: 0.6684 - val_recall: 0.9808\n",
      "Epoch 15/60\n",
      "105678/105678 - 7s - loss: 0.3256 - accuracy: 0.9224 - auc: 0.9449 - precision: 0.8916 - recall: 0.9617 - val_loss: 0.3631 - val_accuracy: 0.8998 - val_auc: 0.9442 - val_precision: 0.6829 - val_recall: 0.9324\n",
      "Epoch 16/60\n",
      "105678/105678 - 7s - loss: 0.3270 - accuracy: 0.9218 - auc: 0.9444 - precision: 0.8912 - recall: 0.9608 - val_loss: 0.3980 - val_accuracy: 0.9009 - val_auc: 0.9456 - val_precision: 0.6752 - val_recall: 0.9736\n",
      "Epoch 17/60\n",
      "105678/105678 - 6s - loss: 0.3269 - accuracy: 0.9223 - auc: 0.9445 - precision: 0.8912 - recall: 0.9620 - val_loss: 0.4376 - val_accuracy: 0.8963 - val_auc: 0.9487 - val_precision: 0.6606 - val_recall: 0.9916\n",
      "Epoch 18/60\n",
      "105678/105678 - 7s - loss: 0.3273 - accuracy: 0.9218 - auc: 0.9443 - precision: 0.8910 - recall: 0.9610 - val_loss: 0.3953 - val_accuracy: 0.9035 - val_auc: 0.9483 - val_precision: 0.6857 - val_recall: 0.9564\n",
      "Epoch 19/60\n",
      "105678/105678 - 6s - loss: 0.3275 - accuracy: 0.9216 - auc: 0.9442 - precision: 0.8907 - recall: 0.9611 - val_loss: 0.4515 - val_accuracy: 0.8951 - val_auc: 0.9448 - val_precision: 0.6573 - val_recall: 0.9957\n",
      "Epoch 20/60\n",
      "105678/105678 - 6s - loss: 0.3272 - accuracy: 0.9221 - auc: 0.9446 - precision: 0.8914 - recall: 0.9612 - val_loss: 0.4337 - val_accuracy: 0.8939 - val_auc: 0.9399 - val_precision: 0.6562 - val_recall: 0.9876\n",
      "Epoch 21/60\n",
      "105678/105678 - 6s - loss: 0.3261 - accuracy: 0.9226 - auc: 0.9450 - precision: 0.8914 - recall: 0.9626 - val_loss: 0.5204 - val_accuracy: 0.8891 - val_auc: 0.9467 - val_precision: 0.6447 - val_recall: 0.9948\n",
      "Epoch 22/60\n",
      "105678/105678 - 6s - loss: 0.3268 - accuracy: 0.9219 - auc: 0.9442 - precision: 0.8905 - recall: 0.9621 - val_loss: 0.4226 - val_accuracy: 0.8965 - val_auc: 0.9474 - val_precision: 0.6617 - val_recall: 0.9894\n",
      "Epoch 23/60\n",
      "105678/105678 - 6s - loss: 0.3271 - accuracy: 0.9214 - auc: 0.9445 - precision: 0.8905 - recall: 0.9609 - val_loss: 0.5849 - val_accuracy: 0.8655 - val_auc: 0.9468 - val_precision: 0.5985 - val_recall: 0.9977\n",
      "Epoch 24/60\n",
      "105678/105678 - 7s - loss: 0.3262 - accuracy: 0.9225 - auc: 0.9450 - precision: 0.8915 - recall: 0.9620 - val_loss: 0.3462 - val_accuracy: 0.8986 - val_auc: 0.9465 - val_precision: 0.6910 - val_recall: 0.8931\n",
      "Epoch 25/60\n",
      "105678/105678 - 7s - loss: 0.3256 - accuracy: 0.9222 - auc: 0.9449 - precision: 0.8916 - recall: 0.9612 - val_loss: 0.3584 - val_accuracy: 0.8972 - val_auc: 0.9442 - val_precision: 0.6887 - val_recall: 0.8879\n",
      "Epoch 26/60\n",
      "105678/105678 - 7s - loss: 0.3262 - accuracy: 0.9221 - auc: 0.9449 - precision: 0.8912 - recall: 0.9615 - val_loss: 0.3728 - val_accuracy: 0.8989 - val_auc: 0.9438 - val_precision: 0.6822 - val_recall: 0.9275\n",
      "Epoch 27/60\n",
      "105678/105678 - 7s - loss: 0.3260 - accuracy: 0.9218 - auc: 0.9446 - precision: 0.8906 - recall: 0.9617 - val_loss: 0.5801 - val_accuracy: 0.8591 - val_auc: 0.9370 - val_precision: 0.5894 - val_recall: 0.9770\n",
      "Epoch 28/60\n",
      "105678/105678 - 7s - loss: 0.3273 - accuracy: 0.9210 - auc: 0.9450 - precision: 0.8910 - recall: 0.9595 - val_loss: 0.3546 - val_accuracy: 0.9039 - val_auc: 0.9493 - val_precision: 0.6859 - val_recall: 0.9596\n",
      "Epoch 29/60\n",
      "105678/105678 - 7s - loss: 0.3285 - accuracy: 0.9216 - auc: 0.9443 - precision: 0.8912 - recall: 0.9605 - val_loss: 0.3768 - val_accuracy: 0.8943 - val_auc: 0.9426 - val_precision: 0.6715 - val_recall: 0.9245\n",
      "Epoch 30/60\n",
      "105678/105678 - 7s - loss: 0.3276 - accuracy: 0.9216 - auc: 0.9443 - precision: 0.8907 - recall: 0.9611 - val_loss: 0.4453 - val_accuracy: 0.8903 - val_auc: 0.9448 - val_precision: 0.6467 - val_recall: 0.9975\n",
      "Epoch 31/60\n",
      "105678/105678 - 6s - loss: 0.3266 - accuracy: 0.9217 - auc: 0.9446 - precision: 0.8910 - recall: 0.9610 - val_loss: 0.3754 - val_accuracy: 0.8946 - val_auc: 0.9412 - val_precision: 0.6614 - val_recall: 0.9706\n",
      "Epoch 32/60\n",
      "105678/105678 - 6s - loss: 0.3275 - accuracy: 0.9216 - auc: 0.9446 - precision: 0.8913 - recall: 0.9603 - val_loss: 0.3724 - val_accuracy: 0.8990 - val_auc: 0.9452 - val_precision: 0.6809 - val_recall: 0.9329\n",
      "Epoch 33/60\n",
      "105678/105678 - 6s - loss: 0.3266 - accuracy: 0.9222 - auc: 0.9445 - precision: 0.8913 - recall: 0.9615 - val_loss: 0.4436 - val_accuracy: 0.8974 - val_auc: 0.9453 - val_precision: 0.6625 - val_recall: 0.9939\n",
      "Epoch 34/60\n",
      "105678/105678 - 7s - loss: 0.3269 - accuracy: 0.9214 - auc: 0.9448 - precision: 0.8908 - recall: 0.9607 - val_loss: 0.3376 - val_accuracy: 0.8999 - val_auc: 0.9474 - val_precision: 0.6921 - val_recall: 0.9008\n",
      "Epoch 35/60\n",
      "105678/105678 - 6s - loss: 0.3261 - accuracy: 0.9222 - auc: 0.9450 - precision: 0.8910 - recall: 0.9622 - val_loss: 0.4039 - val_accuracy: 0.8991 - val_auc: 0.9488 - val_precision: 0.6690 - val_recall: 0.9819\n",
      "Epoch 36/60\n",
      "105678/105678 - 6s - loss: 0.3244 - accuracy: 0.9225 - auc: 0.9450 - precision: 0.8910 - recall: 0.9628 - val_loss: 0.4283 - val_accuracy: 0.9005 - val_auc: 0.9466 - val_precision: 0.6733 - val_recall: 0.9779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/60\n",
      "105678/105678 - 6s - loss: 0.3285 - accuracy: 0.9212 - auc: 0.9444 - precision: 0.8909 - recall: 0.9600 - val_loss: 0.3754 - val_accuracy: 0.9002 - val_auc: 0.9457 - val_precision: 0.6751 - val_recall: 0.9672\n",
      "Epoch 38/60\n",
      "105678/105678 - 6s - loss: 0.3248 - accuracy: 0.9225 - auc: 0.9447 - precision: 0.8912 - recall: 0.9625 - val_loss: 0.3503 - val_accuracy: 0.9030 - val_auc: 0.9486 - val_precision: 0.6961 - val_recall: 0.9155\n",
      "Epoch 39/60\n",
      "105678/105678 - 6s - loss: 0.3263 - accuracy: 0.9216 - auc: 0.9449 - precision: 0.8912 - recall: 0.9605 - val_loss: 0.4426 - val_accuracy: 0.8959 - val_auc: 0.9487 - val_precision: 0.6582 - val_recall: 0.9986\n",
      "Epoch 40/60\n",
      "105678/105678 - 6s - loss: 0.3267 - accuracy: 0.9212 - auc: 0.9450 - precision: 0.8910 - recall: 0.9599 - val_loss: 0.3727 - val_accuracy: 0.8995 - val_auc: 0.9462 - val_precision: 0.6728 - val_recall: 0.9697\n",
      "Epoch 41/60\n",
      "105678/105678 - 6s - loss: 0.3271 - accuracy: 0.9219 - auc: 0.9448 - precision: 0.8911 - recall: 0.9612 - val_loss: 0.3503 - val_accuracy: 0.8967 - val_auc: 0.9456 - val_precision: 0.6877 - val_recall: 0.8873\n",
      "Epoch 42/60\n",
      "105678/105678 - 6s - loss: 0.3265 - accuracy: 0.9216 - auc: 0.9447 - precision: 0.8908 - recall: 0.9610 - val_loss: 0.5011 - val_accuracy: 0.8867 - val_auc: 0.9477 - val_precision: 0.6388 - val_recall: 0.9995\n",
      "Epoch 43/60\n",
      "105678/105678 - 6s - loss: 0.3254 - accuracy: 0.9218 - auc: 0.9448 - precision: 0.8909 - recall: 0.9615 - val_loss: 0.3940 - val_accuracy: 0.8987 - val_auc: 0.9471 - val_precision: 0.6700 - val_recall: 0.9738\n",
      "Epoch 44/60\n",
      "105678/105678 - 7s - loss: 0.3281 - accuracy: 0.9210 - auc: 0.9443 - precision: 0.8905 - recall: 0.9601 - val_loss: 0.4604 - val_accuracy: 0.8943 - val_auc: 0.9462 - val_precision: 0.6547 - val_recall: 0.9993\n",
      "Epoch 45/60\n",
      "105678/105678 - 6s - loss: 0.3258 - accuracy: 0.9215 - auc: 0.9447 - precision: 0.8908 - recall: 0.9607 - val_loss: 0.4429 - val_accuracy: 0.8949 - val_auc: 0.9416 - val_precision: 0.6559 - val_recall: 0.9995\n",
      "Epoch 46/60\n",
      "105678/105678 - 7s - loss: 0.3279 - accuracy: 0.9215 - auc: 0.9445 - precision: 0.8915 - recall: 0.9599 - val_loss: 0.4017 - val_accuracy: 0.8962 - val_auc: 0.9465 - val_precision: 0.6630 - val_recall: 0.9801\n",
      "Epoch 47/60\n",
      "105678/105678 - 8s - loss: 0.3253 - accuracy: 0.9226 - auc: 0.9450 - precision: 0.8919 - recall: 0.9618 - val_loss: 0.4576 - val_accuracy: 0.8955 - val_auc: 0.9451 - val_precision: 0.6574 - val_recall: 0.9982\n",
      "Epoch 48/60\n",
      "105678/105678 - 6s - loss: 0.3262 - accuracy: 0.9221 - auc: 0.9446 - precision: 0.8910 - recall: 0.9619 - val_loss: 0.4669 - val_accuracy: 0.8931 - val_auc: 0.9446 - val_precision: 0.6550 - val_recall: 0.9855\n",
      "Epoch 49/60\n",
      "105678/105678 - 6s - loss: 0.3271 - accuracy: 0.9204 - auc: 0.9449 - precision: 0.8906 - recall: 0.9587 - val_loss: 0.4080 - val_accuracy: 0.8977 - val_auc: 0.9480 - val_precision: 0.6647 - val_recall: 0.9869\n",
      "Epoch 50/60\n",
      "105678/105678 - 7s - loss: 0.3266 - accuracy: 0.9223 - auc: 0.9446 - precision: 0.8916 - recall: 0.9614 - val_loss: 0.3477 - val_accuracy: 0.9013 - val_auc: 0.9478 - val_precision: 0.6893 - val_recall: 0.9239\n",
      "Epoch 51/60\n",
      "105678/105678 - 7s - loss: 0.3277 - accuracy: 0.9213 - auc: 0.9445 - precision: 0.8906 - recall: 0.9606 - val_loss: 0.3545 - val_accuracy: 0.8979 - val_auc: 0.9455 - val_precision: 0.6925 - val_recall: 0.8816\n",
      "Epoch 52/60\n",
      "105678/105678 - 6s - loss: 0.3261 - accuracy: 0.9217 - auc: 0.9448 - precision: 0.8908 - recall: 0.9611 - val_loss: 0.4808 - val_accuracy: 0.8823 - val_auc: 0.9471 - val_precision: 0.6321 - val_recall: 0.9873\n",
      "Epoch 53/60\n",
      "105678/105678 - 7s - loss: 0.3259 - accuracy: 0.9214 - auc: 0.9448 - precision: 0.8908 - recall: 0.9606 - val_loss: 0.4269 - val_accuracy: 0.8965 - val_auc: 0.9470 - val_precision: 0.6605 - val_recall: 0.9941\n",
      "Epoch 54/60\n",
      "105678/105678 - 6s - loss: 0.3252 - accuracy: 0.9221 - auc: 0.9450 - precision: 0.8908 - recall: 0.9621 - val_loss: 0.4060 - val_accuracy: 0.8982 - val_auc: 0.9456 - val_precision: 0.6652 - val_recall: 0.9901\n",
      "Epoch 55/60\n",
      "105678/105678 - 6s - loss: 0.3262 - accuracy: 0.9218 - auc: 0.9447 - precision: 0.8907 - recall: 0.9616 - val_loss: 0.4298 - val_accuracy: 0.8955 - val_auc: 0.9491 - val_precision: 0.6576 - val_recall: 0.9975\n",
      "Epoch 56/60\n",
      "105678/105678 - 6s - loss: 0.3258 - accuracy: 0.9219 - auc: 0.9450 - precision: 0.8912 - recall: 0.9610 - val_loss: 0.3630 - val_accuracy: 0.8970 - val_auc: 0.9467 - val_precision: 0.6833 - val_recall: 0.9053\n",
      "Epoch 57/60\n",
      "105678/105678 - 7s - loss: 0.3266 - accuracy: 0.9223 - auc: 0.9447 - precision: 0.8909 - recall: 0.9624 - val_loss: 0.3358 - val_accuracy: 0.8982 - val_auc: 0.9475 - val_precision: 0.7102 - val_recall: 0.8310\n",
      "Epoch 58/60\n",
      "105678/105678 - 6s - loss: 0.3278 - accuracy: 0.9210 - auc: 0.9443 - precision: 0.8902 - recall: 0.9604 - val_loss: 0.4265 - val_accuracy: 0.9018 - val_auc: 0.9479 - val_precision: 0.6767 - val_recall: 0.9756\n",
      "Epoch 59/60\n",
      "105678/105678 - 6s - loss: 0.3265 - accuracy: 0.9225 - auc: 0.9447 - precision: 0.8912 - recall: 0.9625 - val_loss: 0.3514 - val_accuracy: 0.9001 - val_auc: 0.9462 - val_precision: 0.6760 - val_recall: 0.9629\n",
      "Epoch 60/60\n",
      "105678/105678 - 6s - loss: 0.3257 - accuracy: 0.9217 - auc: 0.9448 - precision: 0.8911 - recall: 0.9607 - val_loss: 0.4581 - val_accuracy: 0.8905 - val_auc: 0.9493 - val_precision: 0.6468 - val_recall: 0.9986\n"
     ]
    }
   ],
   "source": [
    "# train the model with the best configuration\n",
    "epochs = 60\n",
    "\n",
    "with open(\"models/dnw_params.json\",\"r\") as f:\n",
    "    best_config_nn = json.load(f)\n",
    "print(\"Best configuration for the Deep and Wide network:\")\n",
    "print(best_config_nn)\n",
    "\n",
    "model = make_nn_model(best_config_nn)\n",
    "\n",
    "history = model.fit(\n",
    "            train_X,\n",
    "            train_y,\n",
    "            epochs=epochs,\n",
    "            verbose=2,\n",
    "            validation_data=(val_X, val_y),\n",
    "            callbacks=[tf.keras.callbacks.ModelCheckpoint(\"./models/model_checkpoints/mymodel_{epoch}.h5\"),\n",
    "                       tf.keras.callbacks.TensorBoard(log_dir=\"./models/logs\", histogram_freq=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x142d261d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model at epoch 58\n",
    "saved_model = make_nn_model(best_config_nn)\n",
    "saved_model.load_weights(\"./models/model_checkpoints/mymodel_58\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training metrics:\n",
      "loss: 0.3158055896661029\n",
      "accuracy: 0.93259716\n",
      "auc_1: 0.946904\n",
      "precision_1: 0.89056146\n",
      "recall_1: 0.9864116\n",
      "-------------\n",
      "Test metrics:\n",
      "loss: 0.40084671764323715\n",
      "accuracy: 0.89994115\n",
      "auc_1: 0.9477904\n",
      "precision_1: 0.6708705\n",
      "recall_1: 0.98398376\n"
     ]
    }
   ],
   "source": [
    "# check how the model is performing on training and test data\n",
    "train_results = saved_model.evaluate(train_X,train_y,verbose=0)\n",
    "train_metrics = list(zip(saved_model.metrics_names,train_results))\n",
    "test_results = saved_model.evaluate(test_X,test_y,verbose=0)\n",
    "test_metrics = list(zip(saved_model.metrics_names,test_results))\n",
    "\n",
    "print(\"Training metrics:\")\n",
    "for metric in train_metrics:\n",
    "    print(metric[0]+\":\",metric[1])\n",
    "print(\"-------------\")\n",
    "print(\"Test metrics:\")\n",
    "for metric in test_metrics:\n",
    "    print(metric[0]+\":\",metric[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAF1CAYAAADhgoKhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3de7xVdZ3/8ddHDoEXvB6mSSHBDO+MKEpeQPNaTpOT080LeCEdtUzTmsmZfmrT6DQjjU5aFmVqZnnLSXNyKjUEzRuoowmShigIIuCVjAT9/v5Y6+B2c87hcPY+e5/D9/V8PM6Dvddae63PXt+9F+/93d+1dqSUkCRJknKwXrMLkCRJkhrF8CtJkqRsGH4lSZKUDcOvJEmSsmH4lSRJUjYMv5IkScqG4VfZiIgrI+Jfy9tjI2J2g7abImLbBmxnWLmtlm4+vsM6I+LoiPhVe8tGxHci4v91r+q1rvGUiFgUEcsiYotGbLOdGvaPiPnN2Haj5fRcm63y+FSHdd0WEcd2MK+m40RPWZvXWkScFxE/6umatO4y/KpXiYi5EfGnMtwsKv9D2Kje20kpTUspbdeFeo6LiLvrvf2+JqV0TUrpkA7mnZxS+hr0bFiKiP7AfwKHpJQ2Sikt7YntqPcqQ8+KiHit/Pt9RFwaEe9pdm09JSJayuPhmIppR5cBtnraEwAppQ+nlK5qRr1SX2D4VW/0NymljYDdgNHAV6oX6G29Fo2Q43Ou8m5gIPB4swvpyyKiX7NrqNF1KaVBwObAx4C/BGasqwE4pbQSuBcYVzF5HPBEO9OmNrA0qc8y/KrXSik9B9wG7Ayrvmr/bEQ8CTxZTvtIRDwSES9HxG8jYmTb4yNiVEQ8VPYQXUcRnNrmvaOHMiKGRsRNEbE4IpaWvUk7AN8B9ip7Xl4ulx0QEZMi4tmyd/o7EbF+xbq+FBELI2JBRJzQ2XOMiCkR8W8R8UBEvBoRN0fE5uW8tq8nJ0bEs8CdEbFeRHwlIp6JiBci4ocRsUnVak8ot70wIr5Ysa09I+Lecl8tLJ/ju6oee1hEzImIJRFxYUSsVz62wx7wtq9rI2LDsr22LPfXsojYMiJerxyiEBG7lfu5fzvrGhARF5f1LyhvD4iIEUDbMJWXI+LOdh7btr+OLdtmSUT885rW3UnbHBYRM8vXz3OV+7Kcf1bZBgsj4viK6ZuU7bK4bKevVOzHZyJi9/J2W+/dTuX9iRHxs0728Xci4tdlPXdFxNYV87cv570YEbMj4pNVj70sIn4REX8EPtjO+jePiCvK/fJSJ3V8OSL+UNYwMyI+VjFv27KuV8p9f105PSLionJfvRoRj0XEzh3t965KKa1IKT0OfApYDJxVUUtnx4UtI+KnZfs8HRGfr5h3XkTcGBHXlc/xoYj4q45qiIj/ioh55fOaERFjq9Z1fflaeC0iHo+I0RXzOzw+tWMq7wy6Y4F/b2fa1HLdUyLiM+XtflEcr5ZExBzgr6uewyYRcXn5On4uivdyux+Qyud0Q0T8qKz7sYgYERFnl+07LyIOqVh+y4i4pXxdPhURJ1bMW798bb4UETOBPaq21WE7SbUy/KrXioihwGHAwxWT/xYYA+wYEaOAHwB/D2wBfBe4JYqQ8y7gZ8DVFD1ENwB/18F2+gG3As8Aw4CtgGtTSrOAk4F7y6/ZNy0f8nVgBLArsG25/Dnluj4EfBE4GHg/cFAXnuoE4ATgPcBK4JtV8/cDdgAOBY4r/z4IbANsBFxatfwHy20fAvxjRLTV8CbwBaAV2As4EDi16rEfo+ht3w04vKyrS1JKfwQ+DCwo99dGKaUFwBTgkxWLjqfYvyvaWc0/Ax+g2Ld/BewJfCWl9Htgp3KZTVNKB3RSyr7AduXzOyeKDzEdrruT9VwO/H3Zy7gzUBm4/xLYhKLtJwLfiojNynmXlPO2oWi7CUBbOL4L2L+8vR8wh7cDzH7l/I4cDXyNov0eAa4BiOJDx6+BHwN/AXwa+HZE7Fjx2KOA84FBQHsfYq4GNqDYx38BXNRBDX+gCFmbAF8FfhRv97h+DfgVsBkwpNwPULwOx1G8ZzaheC3UbchKSulN4OayLtZwXFgP+DnwfxRtdyBwRkQcWrHKwymOF5tT7NOfRTsf1EoPUrye2pa9ISIqQ+xHgWuBTYFbKN+ra3N8Kk0F9oniw28rsCFwPbBnxbQdaL/n90TgI8Aoivf2x6vmX0lx3Nm2XOYQ4DOd1PI3Zd2bURybf0mRJbYC/oVif7e5FpgPbFlu94KIaHvvngu8r/w7FFg1RrmL7SR1X0rJP/96zR8wF1gGvEwRRr8NrF/OS8ABFcteBnyt6vGzKULEOGABEBXzfgv8a3l7f2B+eXsvip6jlnbqOQ64u+J+AH8E3lcxbS/g6fL2D4CvV8wbUda9bQfPd0rV8jsCbwD9KIJ4ArapmH8HcGrF/e2AFUBLxfLbV8z/D+DyDrZ9BvDfFfcT8KGK+6cCd3SwH1Y9J4r/PFfbrxXLfgq4p7zdD3ge2LODmv4AHFZx/1Bgbnm77fmt1k5V84dUTHsA+PSa1t3B+p6lCFAbV03fH/hTZR3ACxTBul/ZfjtWzPt7YEp5eyJwS3l7FkXIuLa8/wywWwe1XNm2XHl/I4oPM0PL/TutavnvAudWPPaHnTzP9wBvAZu1M2+19qya/whweHn7h8Dkyv1fTj8A+H25f9brynGgk+2dB/yoneknA0+Wtzs7LowBnq2adzZwRcX676uYtx6wEBjbxfpeAv6qYl23V8zbEfhTebvT41M76x0ILKf40PYx4Jpy+n0V056uWH4K8Jny9p3AyRXzDqF8H1EMJfoz5TG2nH8k8JtO9v+vK+7/DcXxul95f1C57k3L1+abwKCK5f8NuLK8PYd3Hm9O4u1jclfaabXXgX/+dfXPnl/1Rn+bUto0pbR1SunUlNKfKubNq7i9NXBW+dXmy1EMSxhK0cuwJfBcSilVLP9MB9sbCjyTirF1azKYoodsRsU2/7ecTrndyho72mal6uX7U/TutTd/y6p1PsPb/4l1tL4tAcqvJ2+NiOcj4lXggqrtdPjYGt1M0VM/nKJH/JWU0gMdLNve81vbGp6vuP06RVDsdN0R8U/x9lCN75Tz/47im4dnyq/z96p47NKq10vbdlop2q96O1uVt+8Cxpa9pf0oeu/2iYhhFL2ij3TyvFa1TUppGfBiWf/WwJiq98HRFL3Tqz22HUOBF1NKL3WyDAARMaFiOMHLFD3iba+hf6D4cPhA+RX/CWWtd1L0eH4LeCEiJkfExu2se2xFG6ztuO6tKPYHdH5c2JpiWE7lvH+ig/dPSukt3u65bG9/fDEiZkUx1ONlijasfE9VvxYHRjF2f22OT6SUllN8kBtX/k0rZ91dMa2j8b6dHZO2pni9LqzYH9+l6P3vyKKK238ClqSi973tPhTvhS0pXlevVW277b2wprrW1E5Stxl+1ddU/mcxDzi/DMptfxuklH5C0VuzVURExfLv7WCd84D3RvsnlKWq+0soDvA7VWxzk1ScoEe53aFd2Gal6uVXlNtpr4YFFP8xVC6/knf+h1S9vgXl7csoTpJ5f0ppY4r/TCr3T2eP7arq/dX2H/f1wDEUQx6u7uTx7T2/ta1hrdedUrogvT1U4+Ry2oMppcMpgsDPyuewJkso2q96O8+V63yKIgSdBkxNKb1KEZBOouhZf6uTda9qmyiugLJ5Wf884K6q98FGKaVTKh67WrtUmAdsHhGbdrIMUYwx/h7wOWCLVAwD+h3layil9HxK6cSU0pYUvd3fjvJyeCmlb6aUdqfo/RwBfKl6/am4AktbG+xUPb+Tutaj6IFsC4SdHRfmUfSQVs4blFI6rGKVQ6vWPYR2XoNRjO/9B4phHJuV++MVVn9PtWdtjk9t2sb9jq14rtMqpnUUfjs7Js2j6PltrdgfG6/N/u/EAorX1aCqbT/XxbrW1E5Stxl+1Zd9Dzg5IsZEYcOI+OvyYHsvRSj8fET0j4gjKMZ4tucBigPx18t1DIyIfcp5i4Ah5Ri9tp6g7wEXRcRfAETEVhVj0a4HjouIHSNiA4pxbWtyTMXy/wLcWNGTUu0nwBciYngZgC6gOPu9shfy/0XEBlGcSHU8cF05fRDwKrAsIrYHKsNRmy9FxGbleOvTKx7bVYuALWL1k/B+SDF04qN0Hn5/AnwlIgaX4xjPAep1Pc8urzsi3hXFCWmbpGJs8qsUQwM6Vbbb9cD5ETGoDIxnVm3nLooA2Ta+d0rV/Y4cFhH7lq/Fr1F8PT+PYrz6iIgYX77W+0fEHhVjnddU80KKExW/XbZ9/4gY186iG1KE6MUAUZzkt+rEtYj4REQMKe++VC77VlnLmHLc7B8pvr5f475ckyguAbYDRbv+JcVl8KDz48IDwGsR8Y9RnHDVLyJ2jojKk612j4gjyg/DZ1CEw/vaKWEQxTFmMdASEecAq/Vod2Btjk9tplKM5x8KzCyn3UMxNGVXOg6/15fbGRLFuPQvt80o2/5XwDciYuMoxg+/LyL26+Lz6FD52vwt8G/lMXUkxbCftvfC9cDZ5WtuCMUHwjZdaSep2wy/6rNSStMpTua4lOI/26coAhYppTeAI8r7L1KMi7ypg/W8SdFztC3FOM/55fJQjJd7HHg+Itp6Y/+x3NZ9UQwfuJ1i7C0ppduAi8vHPcU7T5LqyNUU4zKfpxjb19lZzT8ol58KPE0RJE6rWuauctt3AJNSSm0/TvFFihOfXqMICO0F25uBGRRfv/8PxUlfXZZSeoIijMwpv67cspx+D0XgeSil1NlQkH8FpgOPAo8BD5XT6mFt1z0emFu28ckUQwm64jSKkDeH4mvpH1O0W5u7KILT1A7ud+THFB+mXgR2p+hJp/xa+RCKE90WULyO/h3o8EoW7RhP0WP9BMX45TOqF0gpzQS+QRHcFgG7UISvNnsA90fEMoqTu05PKc2hCITfo3iPPkNxstuFa1FbtU+V23il3M5SYPdUnFy5puPCmxQnf+1K8f5ZAnyfYrhCm5sp3v8vUeyXI1L7J2f+kmLI0+/L57WczoeXrLI2x6cKvy3rvL9tuERKaQlF+H4hpfRkB4/7Xlnr/1G85qu3MwF4F0Wgfgm4kWIceD0cSTEWfwHw3xTj0G8v532VYr89TRHAV30o7mI7Sd0W7xxyJKmRImIKxYkb3292LT0tisuT/TiH51pvEXElxclAnV2dQjWKiPMoTuQ8ptm1SOo5uV80X1IDlF9Xtl0+TZKkpnHYg6QeFRFXUQwNOaPqzG9JkhrOYQ+SJEnKhj2/kiRJyobhV5IkSdlo6Alvra2tadiwYY3cpCRJkjIzY8aMJSmlwe3Na2j4HTZsGNOnT2/kJiVJkpSZiOjwmvIOe5AkSVI2DL+SJEnKhuFXkiRJ2TD8SpIkKRuGX0mSJGXD8CtJkqRsGH4lSZKUDcOvJEmSsrHG8BsRP4iIFyLidxXTNo+IX0fEk+W/m/VsmZIkSVLtutLzeyXwoappXwbuSCm9H7ijvC9JkiT1amsMvymlqcCLVZMPB64qb18F/G2d65IkSZLqrrtjft+dUlpY3n4eeHed6pEkSZJ6TM0nvKWUEpA6mh8RJ0XE9IiYvnjx4lo3J0mSJHVbSzcftygi3pNSWhgR7wFe6GjBlNJkYDLA6NGjOwzJPen2mYuasVkADtrRTnFJkqTeors9v7cAx5a3jwVurk85kiRJUs/pyqXOfgLcC2wXEfMjYiLwdeDgiHgSOKi8L0mSJPVqaxz2kFI6soNZB9a5FkmSJKlH+QtvkiRJyobhV5IkSdkw/EqSJCkbhl9JkiRlw/ArSZKkbBh+JUmSlA3DryRJkrJh+JUkSVI2DL+SJEnKhuFXkiRJ2TD8SpIkKRuGX0mSJGXD8CtJkqRstDS7AEmSJDXA7Nsav83tPtz4ba6BPb+SJEnKhuFXkiRJ2TD8SpIkKRuGX0mSJGXD8CtJkqRsGH4lSZKUDcOvJEmSsmH4lSRJUjYMv5IkScqG4VeSJEnZMPxKkiQpG4ZfSZIkZcPwK0mSpGwYfiVJkpQNw68kSZKyYfiVJElSNgy/kiRJyobhV5IkSdkw/EqSJCkbhl9JkiRlw/ArSZKkbBh+JUmSlA3DryRJkrJh+JUkSVI2DL+SJEnKhuFXkiRJ2TD8SpIkKRuGX0mSJGXD8CtJkqRsGH4lSZKUDcOvJEmSsmH4lSRJUjYMv5IkScqG4VeSJEnZMPxKkiQpG4ZfSZIkZcPwK0mSpGwYfiVJkpQNw68kSZKyYfiVJElSNmoKvxHxhYh4PCJ+FxE/iYiB9SpMkiRJqrduh9+I2Ar4PDA6pbQz0A/4dL0KkyRJkuqt1mEPLcD6EdECbAAsqL0kSZIkqWd0O/ymlJ4DJgHPAguBV1JKv6peLiJOiojpETF98eLF3a9UkiRJqlEtwx42Aw4HhgNbAhtGxDHVy6WUJqeURqeURg8ePLj7lUqSJEk1qmXYw0HA0ymlxSmlFcBNwN71KUuSJEmqv1rC77PAByJig4gI4EBgVn3KkiRJkuqvljG/9wM3Ag8Bj5XrmlynuiRJkqS6a6nlwSmlc4Fz61SLJEmS1KP8hTdJkiRlw/ArSZKkbBh+JUmSlA3DryRJkrJh+JUkSVI2DL+SJEnKhuFXkiRJ2TD8SpIkKRuGX0mSJGXD8CtJkqRsGH4lSZKUDcOvJEmSsmH4lSRJUjYMv5IkScqG4VeSJEnZMPxKkiQpG4ZfSZIkZcPwK0mSpGwYfiVJkpQNw68kSZKyYfiVJElSNgy/kiRJyobhV5IkSdkw/EqSJCkbhl9JkiRlw/ArSZKkbBh+JUmSlA3DryRJkrJh+JUkSVI2DL+SJEnKhuFXkiRJ2TD8SpIkKRuGX0mSJGXD8CtJkqRsGH4lSZKUDcOvJEmSsmH4lSRJUjYMv5IkScqG4VeSJEnZMPxKkiQpG4ZfSZIkZcPwK0mSpGwYfiVJkpQNw68kSZKyYfiVJElSNgy/kiRJyobhV5IkSdkw/EqSJCkbhl9JkiRlw/ArSZKkbBh+JUmSlA3DryRJkrJh+JUkSVI2agq/EbFpRNwYEU9ExKyI2KtehUmSJEn11lLj4/8L+N+U0scj4l3ABnWoSZIkSeoR3Q6/EbEJMA44DiCl9AbwRn3KkiRJkuqvlmEPw4HFwBUR8XBEfD8iNqxTXZIkSVLd1RJ+W4DdgMtSSqOAPwJfrl4oIk6KiOkRMX3x4sU1bE6SJEmqTS3hdz4wP6V0f3n/Roow/A4ppckppdEppdGDBw+uYXOSJElSbbodflNKzwPzImK7ctKBwMy6VCVJkiT1gFqv9nAacE15pYc5wPG1lyRJkiT1jJrCb0rpEWB0nWqRJEmSepS/8CZJkqRsGH4lSZKUDcOvJEmSsmH4lSRJUjYMv5IkScqG4VeSJEnZMPxKkiQpG4ZfSZIkZcPwK0mSpGwYfiVJkpQNw68kSZKyYfiVJElSNgy/kiRJyobhV5IkSdkw/EqSJCkbhl9JkiRlw/ArSZKkbBh+JUmSlA3DryRJkrJh+JUkSVI2DL+SJEnKhuFXkiRJ2TD8SpIkKRuGX0mSJGXD8CtJkqRsGH4lSZKUDcOvJEmSsmH4lSRJUjYMv5IkScqG4VeSJEnZMPxKkiQpG4ZfSZIkZcPwK0mSpGwYfiVJkpQNw68kSZKyYfiVJElSNgy/kiRJyobhV5IkSdkw/EqSJCkbhl9JkiRlw/ArSZKkbBh+JUmSlA3DryRJkrJh+JUkSVI2WppdgCRJUpZm39bsCrJkz68kSZKyYfiVJElSNgy/kiRJyobhV5IkSdkw/EqSJCkbhl9JkiRlw/ArSZKkbBh+JUmSlA3DryRJkrJRc/iNiH4R8XBE3FqPgiRJkqSeUo+e39OBWXVYjyRJktSjagq/ETEE+Gvg+/UpR5IkSeo5tfb8Xgz8A/BWRwtExEkRMT0ipi9evLjGzUmSJEnd1+3wGxEfAV5IKc3obLmU0uSU0uiU0ujBgwd3d3OSJElSzWrp+d0H+GhEzAWuBQ6IiB/VpSpJkiSpB3Q7/KaUzk4pDUkpDQM+DdyZUjqmbpVJkiRJdeZ1fiVJkpSNlnqsJKU0BZhSj3VJkiRJPcWeX0mSJGXD8CtJkqRsGH4lSZKUDcOvJEmSsmH4lSRJUjYMv5IkScqG4VeSJEnZMPxKkiQpG4ZfSZIkZcPwK0mSpGwYfiVJkpQNw68kSZKyYfiVJElSNgy/kiRJyobhV5IkSdkw/EqSJCkbhl9JkiRlw/ArSZKkbBh+JUmSlA3DryRJkrJh+JUkSVI2DL+SJEnKRkuzC1jX3T5zUVO2e9CO727KdiVJknoze34lSZKUDcOvJEmSsmH4lSRJUjYMv5IkScqG4VeSJEnZMPxKkiQpG4ZfSZIkZcPwK0mSpGwYfiVJkpQNw68kSZKyYfiVJElSNgy/kiRJyobhV5IkSdkw/EqSJCkbhl9JkiRlw/ArSZKkbBh+JUmSlI2WZhcgSZLUdLNva3YFahB7fiVJkpQNw68kSZKyYfiVJElSNgy/kiRJyobhV5IkSdkw/EqSJCkbhl9JkiRlw/ArSZKkbBh+JUmSlA3DryRJkrJh+JUkSVI2DL+SJEnKRrfDb0QMjYjfRMTMiHg8Ik6vZ2GSJElSvbXU8NiVwFkppYciYhAwIyJ+nVKaWafaJEmSpLrqds9vSmlhSumh8vZrwCxgq3oVJkmSJNVbXcb8RsQwYBRwfzvzToqI6RExffHixfXYnCRJktQttQx7ACAiNgJ+CpyRUnq1en5KaTIwGWD06NGp1u1JkqQMzL6t2RVoHVVTz29E9KcIvteklG6qT0mSJElSz6jlag8BXA7MSin9Z/1KkiRJknpGLT2/+wDjgQMi4pHy77A61SVJkiTVXbfH/KaU7gaijrVIkiRJPcpfeJMkSVI2DL+SJEnKRs2XOlPvdPvMRU3b9kE7vrtp25YkSeqMPb+SJEnKhuFXkiRJ2TD8SpIkKRuGX0mSJGXD8CtJkqRsGH4lSZKUDcOvJEmSsmH4lSRJUjYMv5IkScqG4VeSJEnZMPxKkiQpG4ZfSZIkZaOl2QVIkqRebvZtza5Aqht7fiVJkpQNw68kSZKyYfiVJElSNhzzKyl7K1asYP78+SxfvrzZpfQpAwcOZMiQIfTv37/ZpUhSlxl+JWVv/vz5DBo0iGHDhhERzS6nT0gpsXTpUubPn8/w4cObXY4kdZnDHiRlb/ny5WyxxRYG37UQEWyxxRb2lkvqcwy/kgQG325wn0nqiwy/ktRLnH/++ey0006MHDmSXXfdlfvvv59hw4axZMmSZpcmSesMx/xKUpXbZy6q6/oO2vHda1zm3nvv5dZbb+Whhx5iwIABLFmyhDfeeKOudUiSDL/qAfUODl3VlYAh9VYLFy6ktbWVAQMGANDa2rpq3iWXXMLPf/5zVqxYwQ033MD222/Piy++yAknnMCcOXPYYIMNmDx5MiNHjmSXXXZh2rRpbLLJJrS2tnLRRRcxYcIEJkyYwPjx4zn44IOb9RQlqVdw2IMk9QKHHHII8+bNY8SIEZx66qncddddq+a1trby0EMPccoppzBp0iQAzj33XEaNGsWjjz7KBRdcwIQJEwDYZ599uOeee3j88cfZZpttmDZtGlD0LO+9996Nf2KS1MsYfiWpF9hoo42YMWMGkydPZvDgwXzqU5/iyiuvBOCII44AYPfdd2fu3LkA3H333YwfPx6AAw44gKVLl/Lqq68yduxYpk6dytSpUznllFN47LHHeO6559hss83YcMMNm/HUJKlXMfxKUi/Rr18/9t9/f7761a9y6aWX8tOf/hRg1VCIfv36sXLlyk7XMW7cOKZNm8a0adPYf//9GTx4MDfeeCNjx47t8folqS8w/EpSLzB79myefPLJVfcfeeQRtt566w6XHzt2LNdccw0AU6ZMobW1lY033pihQ4eyZMkSnnzySbbZZhv23XdfJk2axLhx43r8OUhSX+AJb5LUCyxbtozTTjuNl19+mZaWFrbddlsmT57Mrbfe2u7y5513HieccAIjR45kgw024Kqrrlo1b8yYMbz55ptAEZLPPvts9t1334Y8D0nq7SKl1LCNjR49Ok2fPr1h22vTrKsPqLG82oO6a9asWeywww7NLqNPct9lYvZtza5ANXpk3stN2e6uBx3ZlO1GxIyU0uj25jnsQZIkSdlw2IMkSX2JvbBSTez5lSRJUjYMv5IkScqG4VeSJEnZMPxKkiQpG57wpnVGsy5p5yXWVA8RwZlnnsk3vvENACZNmsSyZcs477zzuvT4RYsWMXHiRObNm8eKFSsYNmwYv/jFL5gyZQqTJk3q8HrBkpQbw68kVav32fTbfXiNiwwYMICbbrqJs88+m9bW1rXexDnnnMPBBx/M6aefDsCjjz661uuQpBw47EGSeoGWlhZOOukkLrrootXmzZ07lwMOOICRI0dy4IEH8uyzz662zMKFCxkyZMiq+yNHjlx1e9myZXz84x9n++235+ijj6btx43uuOMORo0axS677MIJJ5zAn//8Zx588EGOOOIIAG6++WbWX3993njjDZYvX84222xT76ctSQ1n+JWkXuKzn/0s11xzDa+88so7pp922mkce+yxPProoxx99NF8/vOfb/exEydO5IMf/CDnn38+CxYsWDXv4Ycf5uKLL2bmzJnMmTOHe+65h+XLl3Pcccdx3XXX8dhjj7Fy5Uouu+wyRo0axSOPPALAtGnT2HnnnXnwwQe5//77GTNmTM/uAElqAMOvJPUSG2+8MRMmTOCb3/zmO6bfe++9HHXUUQCMHz+eu+++e7XHHnroocyZM4cTTzyRJ554glGjRrF48WIA9txzT4YMGcJ6663Hrrvuyty5c5k9ezbDhw9nxIgRABx77LFMnTqVlpYW3ve+9zFr1iweeOABzjzzTKZOncq0adMYO3ZsD+8BSep5jvmVpF7kjDPOYLfdduP4449f68duvvnmHHXUURx11FF85CMfYerUqWyxxRYMGDBg1TL9+vVj5cqVna5n3Lhx3HbbbfTv35+DDjqI4447jjfffJMLL7xwrWvKgr+4JvUp9vxKUi+y+eab87UN5M4AAAaRSURBVMlPfpLLL7981bS9996ba6+9FoBrrrmm3R7YO++8k9dffx2A1157jT/84Q+8973v7XA72223HXPnzuWpp54C4Oqrr2a//fYDYOzYsVx88cXstddeDB48mKVLlzJ79mx23nnnuj1PSWoWe36lGjXrEmvgZdbWVWeddRaXXnrpqvuXXHIJxx9/PBdeeCGDBw/miiuuWO0xM2bM4HOf+xwtLS289dZbfOYzn2GPPfZgypQp7W5j4MCBXHHFFXziE59g5cqV7LHHHpx88skAjBkzhkWLFjFu3DigOHnu+eefJyLq/2QlqcGi7azfRhg9enSaPn16w7bXppnhROpJht/6mDVrFjvssEOzy+iT3Hc47KGbHpn3ctO2vevQTZu27WZp1v7e9aAjm7LdiJiRUhrd3jx7fiVJ6w6D6FprZgjNjfu6d3DMryRJkrJhz68kScqKPbB5M/xKfVizxrOvi2ONU0qe0LWWGnnOyLrOMCY1jsMeJGVv4MCBLF261DC3FlJKLF26lIEDBza7FElaK/b8Slpr69rl3YYMGcL8+fNX/SKaumbgwIEMGTKk2WVI0lqpKfxGxIeA/wL6Ad9PKX29LlVJUgP179+f4cOHN7uMdVM3rr7gEABJPanb4Tci+gHfAg4G5gMPRsQtKaWZ9SpOkqp53e6+pXWBQVZS71LLmN89gadSSnNSSm8A1wKH16csSZIkqf5qGfawFTCv4v58YExt5UhSc7QuuLPZJUiSGqDHT3iLiJOAk8q7yyJidk9vs0IrsKSB21Nz2M55sJ3zYVvnwXbOwlHNauetO5pRS/h9DhhacX9IOe0dUkqTgck1bKfbImJ6R7/rrHWH7ZwH2zkftnUebOc89MZ2rmXM74PA+yNieES8C/g0cEt9ypIkSZLqr9s9vymllRHxOeCXFJc6+0FK6fG6VSZJkiTVWU1jflNKvwB+UadaekJThluo4WznPNjO+bCt82A756HXtXP4c56SJEnKRS1jfiVJkqQ+ZZ0IvxHxoYiYHRFPRcSX25k/ICKuK+ffHxHDGl+latWFdj4zImZGxKMRcUdEdHiZE/Vea2rniuX+LiJSRPSqs4jVNV1p54j4ZPmefjwiftzoGlUfXTh2vzcifhMRD5fH78OaUae6LyJ+EBEvRMTvOpgfEfHN8jXwaETs1ugaK/X58FvxM8sfBnYEjoyIHasWmwi8lFLaFrgI+PfGVqladbGdHwZGp5RGAjcC/9HYKlWrLrYzETEIOB24v7EVqh660s4R8X7gbGCflNJOwBkNL1Q16+J7+ivA9SmlURRXjvp2Y6tUHVwJfKiT+R8G3l/+nQRc1oCaOtTnwy9d+5nlw4Gryts3AgdGRDSwRtVuje2cUvpNSun18u59FNeeVt/S1Z9N/xrFh9jljSxOddOVdj4R+FZK6SWAlNILDa5R9dGVtk7AxuXtTYAFDaxPdZBSmgq82MkihwM/TIX7gE0j4j2NqW5160L4be9nlrfqaJmU0krgFWCLhlSneulKO1eaCNzWoxWpJ6yxncuvy4amlP6nkYWprrryfh4BjIiIeyLivojorFdJvVdX2vo84JiImE9xBanTGlOaGmht/w/vUT3+88ZSo0XEMcBoYL9m16L6ioj1gP8EjmtyKep5LRRfke5P8S3O1IjYJaX0clOrUk84ErgypfSNiNgLuDoidk4pvdXswrRuWhd6frvyM8urlomIFoqvVZY2pDrVS5d+TjsiDgL+GfhoSunPDapN9bOmdh4E7AxMiYi5wAeAWzzprc/pyvt5PnBLSmlFSulp4PcUYVh9S1faeiJwPUBK6V5gINDakOrUKF36P7xR1oXw25WfWb4FOLa8/XHgzuQFjvuaNbZzRIwCvksRfB0f2Dd12s4ppVdSSq0ppWEppWEUY7s/mlKa3pxy1U1dOW7/jKLXl4hopRgGMaeRRaouutLWzwIHAkTEDhThd3FDq1RPuwWYUF714QPAKymlhc0qps8Pe+joZ5Yj4l+A6SmlW4DLKb5GeYpiQPanm1exuqOL7XwhsBFwQ3k+47MppY82rWittS62s/q4LrbzL4FDImIm8CbwpZSS39j1MV1s67OA70XEFyhOfjvODqq+JSJ+QvFhtbUcu30u0B8gpfQdirHchwFPAa8Dxzen0oK/8CZJkqRsrAvDHiRJkqQuMfxKkiQpG4ZfSZIkZcPwK0mSpGwYfiVJkpQNw68kSZKyYfiVJElSNgy/kiRJysb/B1UOD+PAsrF7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plot the distribution of raw probabilities\n",
    "predictions_nn = model.predict(test_X)\n",
    "predictions_nn_df = pd.DataFrame({\"prediction\":predictions_nn.squeeze(),\"ground_truth\":test_y.squeeze()})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "#sns.kdeplot(predictions_nn_df.loc[predictions_nn_df.ground_truth == 0].prediction, shade=1, color='darkturquoise', label = \"Show\")\n",
    "#sns.kdeplot(predictions_nn_df.loc[predictions_nn_df.ground_truth == 1].prediction, shade=1, color='red', label = \"No Show\")\n",
    "ax.hist(x=\"prediction\", data = predictions_nn_df.loc[predictions_nn_df.ground_truth == 0], \n",
    "        bins=20, density=True, label='Show',alpha=0.3)\n",
    "ax.hist(x=\"prediction\", data = predictions_nn_df.loc[predictions_nn_df.ground_truth == 1], \n",
    "        bins=20, density=True, label='No Show', alpha=0.3)\n",
    "ax.set_title(\"Predicted probability of no-show per class - Deep and Wide model\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.93     17664\n",
      "           1       0.65      1.00      0.79      4433\n",
      "\n",
      "    accuracy                           0.89     22097\n",
      "   macro avg       0.82      0.93      0.86     22097\n",
      "weighted avg       0.93      0.89      0.90     22097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(test_y,np.rint(predictions_nn.squeeze()).astype(np.int8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int8)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "\n",
    "The Deep and Wide model shows good performances with an AUC_ROC score which remains stable when moving from the training set to the test set.\n",
    "\n",
    "Overall, the model seems to have found a generalized solution to the problem. Even if a decrease in precision is preminent, this can be explained considering that the test set has not been oversampled to mimick as much as possible real operationals conditions (the training set therefore contains much more no-shows compared to the test set). \n",
    "\n",
    "Moving to the \"business\" metrics, the recall is in both training and test set above 98%, which is a great result. This means that the model is able to identify most of the patients who are not going to show up to medical appointments. \n",
    "The test precision is roughly 67%, which means our hypothetic no-show team will still have to deal with some false positives. However, the number of unnecessary interventions following the model is getting closer to 3 out of 10, which is a significant improvement over the 8 out of 10 (20% precision score) of the baseline model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "In addition to the model built above, I trained an xgboost model to have a comparison with a different type of machine learning model that could potentially solve the problem. XGBoost is the go-to algorithm for most machine learning challenges and it implements gradient boosted decision trees. \n",
    "\n",
    "In order to have a fair comparison with the Deep and Wide network, a hyperparameter tuning job has been performed with the same objective of maximizing validation AUC ROC. The implementation can be found in the file 'models/xgboost_hparams_tuning.py' and it's not reported here for brevity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best configuration for the XGBoost model:\n",
      "{'verbosity': 0, 'num_threads': 2, 'objective': 'binary:logistic', 'booster': 'gbtree', 'eval_metric': ['auc', 'ams@0', 'logloss'], 'colsample_bytree': 0.6449230140228082, 'eta': 0.32949882641198586, 'gamma': 0.17591537269244814, 'grow_policy': 'depthwise', 'max_depth': 6, 'min_child_weight': 1}\n"
     ]
    }
   ],
   "source": [
    "# load the best hyperparameters configuration found during the tuning\n",
    "with open(\"models/xgboost_params.json\",\"r\") as f:\n",
    "    best_config_xgboost = json.load(f)\n",
    "print(\"Best configuration for the XGBoost model:\")\n",
    "print(best_config_xgboost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-auc:0.96754\teval-ams@0:84.34995\teval-logloss:0.47485\n",
      "[1]\teval-auc:0.96645\teval-ams@0:84.62337\teval-logloss:0.37265\n",
      "[2]\teval-auc:0.96728\teval-ams@0:84.74600\teval-logloss:0.30852\n",
      "[3]\teval-auc:0.96714\teval-ams@0:84.66460\teval-logloss:0.27378\n",
      "[4]\teval-auc:0.96835\teval-ams@0:84.61449\teval-logloss:0.24851\n",
      "[5]\teval-auc:0.96863\teval-ams@0:84.86166\teval-logloss:0.23252\n",
      "[6]\teval-auc:0.96835\teval-ams@0:84.95834\teval-logloss:0.22459\n",
      "[7]\teval-auc:0.96801\teval-ams@0:84.99585\teval-logloss:0.21952\n",
      "[8]\teval-auc:0.96881\teval-ams@0:84.82929\teval-logloss:0.21350\n",
      "[9]\teval-auc:0.96913\teval-ams@0:84.80513\teval-logloss:0.21064\n"
     ]
    }
   ],
   "source": [
    "# prepare the data and train xgboost\n",
    "train_set = xgb.DMatrix(train_X, label=train_y)\n",
    "val_set = xgb.DMatrix(val_X, label=val_y)\n",
    "test_set = xgb.DMatrix(test_X, label=test_y)\n",
    "\n",
    "bst = xgb.train(best_config_xgboost, train_set, evals=[(val_set, \"eval\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(bst,set,ground_truth):\n",
    "    \"\"\"Compute the metrics for an xgboost model\"\"\"\n",
    "    preds = bst.predict(set)\n",
    "    pred_labels = np.rint(preds)\n",
    "    results = {\"accuracy\": sklearn.metrics.accuracy_score(ground_truth,pred_labels),\n",
    "             \"precision\": sklearn.metrics.precision_score(ground_truth,pred_labels),\n",
    "             \"recall\": sklearn.metrics.recall_score(ground_truth,pred_labels),\n",
    "             \"AUC_ROC\": sklearn.metrics.roc_auc_score(ground_truth,pred_labels)}\n",
    "    for metric, value in results.items():\n",
    "        print(metric,\"-\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set performance:\n",
      "accuracy - 0.9421450065292681\n",
      "precision - 0.8998305693895364\n",
      "recall - 0.9950604667007324\n",
      "AUC_ROC - 0.9421450065292682\n",
      "\n",
      "Validation set performance:\n",
      "accuracy - 0.9087206408109698\n",
      "precision - 0.6889411764705883\n",
      "recall - 0.9923181201988251\n",
      "AUC_ROC - 0.9400501811451938\n",
      "\n",
      "Test set performance:\n",
      "accuracy - 0.9107118613386432\n",
      "precision - 0.6943127962085308\n",
      "recall - 0.9914279269117979\n",
      "AUC_ROC - 0.940941544977638\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set performance:\")\n",
    "evaluate_model(bst,train_set,train_y)\n",
    "print(\"\\nValidation set performance:\")\n",
    "evaluate_model(bst,val_set,val_y)\n",
    "print(\"\\nTest set performance:\")\n",
    "evaluate_model(bst,test_set,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions_xgb = bst.predict(test_set)\n",
    "predictions_xgb_df = pd.DataFrame({\"prediction\":predictions_xgb.squeeze(),\"ground_truth\":test_y.squeeze()})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "#sns.kdeplot(predictions_xgb_df.loc[predictions_xgb_df.ground_truth == 1].prediction, shade=1, color='red', label = \"No Show\")\n",
    "#sns.kdeplot(predictions_xgb_df.loc[predictions_xgb_df.ground_truth == 0].prediction, shade=1, color='darkturquoise', label = \"Show\")\n",
    "ax.hist(x=\"prediction\", data = predictions_xgb_df.loc[predictions_xgb_df.ground_truth == 0], \n",
    "        bins=20, density=True, label='Show',alpha=0.3)\n",
    "ax.hist(x=\"prediction\", data = predictions_xgb_df.loc[predictions_xgb_df.ground_truth == 1], \n",
    "        bins=20, density=True, label='No Show', alpha=0.3)\n",
    "ax.set_title(\"Predicted probability of no-show per class - XGBoost model\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94     17664\n",
      "           1       0.69      0.99      0.82      4433\n",
      "\n",
      "    accuracy                           0.91     22097\n",
      "   macro avg       0.85      0.94      0.88     22097\n",
      "weighted avg       0.94      0.91      0.92     22097\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_pred_labels = np.rint(predictions_xgb)\n",
    "print(sklearn.metrics.classification_report(test_y,xgb_pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment\n",
    "Also this XGBoost implementation gives some really good results. The precision and recall values are comparable (but higher) with the Deep and Wide network, therefore the same considerations apply. The AUC ROC is higher for the Deep and Wide network (0.948 vs 0.941), although having loaded the model checkpoints for a different epoch could have changed the outcome of this comparison for all three metrics considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "A machine learning approach brings clear advantages in reducing the number of no-shows to medical appointments. The recall score close to 100% of the two models explored in this notebook is comparable to the only advantage of the baseline model: don't miss any potential no-show. \n",
    "\n",
    "At the same time, a machine learning powered approach can cut through the clutter, reducing the number of cases that are currently wrongly considered potential no-show by over 60%. The dataset covers slightly more than a month and contains over 110k appointments. The possibility of reducing the number of appointments to analyze and act upon could make the problem more manageable and more rewarding for the people involved. \n",
    "\n",
    "Possibility for people working on reducing no-shows to interpret the model.\n",
    "\n",
    "Sigmoid shape gives the possibility to move the threshold in more seamless way. (more ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and future steps\n",
    "\n",
    "### Limitations\n",
    "* The dataset only convered a period of slightly over than a month. It would be interesting to repeat the analysis with a longer timeframe\n",
    "* Most of the features are orthogonal to the target variable: what other features could be better predictors of medical appointment no-shows?\n",
    "* Certain variables present issues or unclarified points, for example:\n",
    "    * No SMS were sent for the equivalent of roughly for one third of the available timeframe. It is not clear what might have happened there. Not enough data is available in the dataset to disregard that portion of data.\n",
    "    * There is no indication of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
